# DL
Deep Learning in this 10 years (2011-2020)

Important Persons: Yann LeCun, Geoffrey Hinton, Yoshua Bengio

2011 Yoshua proposed ReLU activation function

Papers:
Deep Sparse Rectifier Neural Networks
Rectifier Nonlinearities Improve Neural Network Acoustic Models
Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
Self-Normalizing Neural Networks
Gaussian Error Linear Units (GELUs)

2012 Hinton: AlexNet

Papers:
ImageNet Classification with Deep Convolutional Neural Networks
ImageNet: A Large-Scale Hierarchical Image Database
Flexible, High Performance Convolutional Neural Networks for Image Classification
Gradient-Based Learning Applied to Document Recognition

2013 DQN

Papers:
Distributed Representations of Words and Phrases and their Compositionality
Playing Atari with Deep Reinforcement Learning
GloVe: Global Vectors for Word Representation
Learning from Delayed Rewards

2014 Bengio & Ian Goodfellow: GAN

Papers:
Generative Adversarial Networks
Neural Machine Translation by Jointly Learning to Align and Translate
Adam:A Method for Stochastic Optimization
Wasserstein GAN & Improved Training of Wasserstein GANs
A Style-Based Generator Architecture for Generative Adversarial Networks
Decoupled Weight Decay Regularization

2015 何恺明: ResNet

Papers:
Deep Residual Learning forImage Recognition 
Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
Going Deeper with Convolutions
Very Deep Convolutional Networks for Large-Scale Image Recognition
Neural Ordinary Differential Equations
Layer Normalization
Instance Normalization: The Missing Ingredient for Fast Stylization
Group Normalization

2016 DeepMind: AlphaGo

Papers:
Mastering the game of Go with deep neuralnetworks and tree search
Mastering the Game of Go without Human Knowledge

2017 Google: Attention is All You Need

Papers:
Attention Is All You Need
Neural Architecture Search with Reinforcement Learning

2018 Google: BERT

Papers:
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Deep contextualized word representations
Improving Language Understanding by Generative Pre-Training
Language Models are Unsupervised Multitask Learners
Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
XLNet: Generalized Autoregressive Pretraining for Language Understanding
Neural Machine Translation of Rare Words with Subword Units

2019 MIT: Deep Double Descent

Papers:
Deep Double Descent: Where Bigger Models and More Data Hurt
The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (https://leogao.dev/2019/12/31/The-Decade-of-Deep-Learning/)
